,title,subtitle,aut_href,author,ema_href,email,date,time,image_path,caption,source,one_p,two_p,three_p,four_p,five_p,tag1,tag2,tag3,tag4,link_n1,next1,link_n2,next2,link_n3,next3,extra1,extra2,extra3,extra4,extra5,extra6,extra7,extra8,extra9,extra10,metatitle 
1,Importance of Reading Code and Technical Papers,Why Reading Other People’s Code and Intense Technical Papers is the Key to your Success as a Developer,/index.html,Noah Caldwell-Gatsos,mailto:ncaldwellgatsos@gmail.com,My Email,8/1/19,12:33PM CST ,/projects/ai/images/debugger.jpg,"An Example of a PyCharm Debugger, It’s a Lot Simpler Than it Looks ",Google Images ,"I get asked a lot how I can find reading fun - I’ve gotten caught at social gatherings flipping through pages of a book (I know, I’m a party animal) and asked if I genuinely enjoy it. In all honesty, it’s astounding to me that less people enjoy reading, since it’s something that I rarely have to think about while I’m doing it. It’s surprising that most people find it a chore. Something that that’s recently given me a fair amount of insight towards others’ opinions on my reading habits is developing my own skills as a programmer. One of the worst things I have to do everyday is read other people’s code. It’s boring, I often get lost from one line to the next, I don’t know what everything means, etc. In other words - it’s probably exactly what the majority of people who remark that my reading habits are strange are thinking when they tackle lengthy online articles or novels. There’s obviously some differences - code is semantically different from plain English - but making that comparison has led to me to believe that there’s a way to make reading code and rough technical papers more manageable. ","What’s that way? The same way that I grew to love reading everyday in a way that other people haven’t: practice. Reading’s an under-appreciated skill. I think it would surprise folks to learn that widespread literacy is a relatively recent phenomenon. World illiteracy didn’t <a href=“https://unesdoc.unesco.org/ark:/48223/pf0000247563”>start rapidly declining </a> until the 1970s. It has historically been a mark of the elite and well-educated (i.e. those with access to resources), but in modern times it’s relatively commonplace in the Western world. You’d be surprised to find someone who grew up speaking English fluently with full access to decent public education and used to surfing the web who wasn’t able to read above a fifth grade level. The age we live in is termed the ‘Information Age’. The point I want to make is that people have begun to see reading as a given, instead of a skill - which I worry is hindering a lot of people’s success, not only in actually reading, but in jobs and skills that require it peripherally, like coding. Another example I can think of is giving up on sight-reading in my piano lessons when it got hard. I was so used to the ease of regular reading that appreciating that sight reading was a different skill entirely was a foreign concept to me. I gave up on it because it wasn’t easy. I don’t want to do the same with code. ","Before I explain how to improve your code, it’s essential that I clarify <b>why</b> it’s important to know how to read code well. As of August 2019, I’m the solo leader on a startup programming project - I built our system from scratch and I only had to clarify inputs and outputs for the other team members working on our  system. I <b>still have to read other people’s code</b> all the time. I have to be able to quickly and fluently understand the inner workings of a variety of different programming packages, GitHub open source hacks, and workarounds developed by other folks in the community everyday just to get my stuff to function properly. If you’re a professional software engineer working with a team and a deadline, this is even more essential. A typical software development team has a variety of programmers from all kinds of backgrounds. You’re going to have to sort through their code, understand it just as well as something of your own, and actually produce results from that understanding. All probably on a deadline. It can’t be something that you view as a chore, or something that’s a pain to get done. It has to be something you’re well versed in and can get excited to do. In other words, it has to be as exciting as reading a semi-decent book. I’m not saying get caught at social gatherings reading code, but try and get your skills to a point where reading code becomes more of a fun exercise than something you wouldn’t touch with a five foot pole. ","That begins my main premise: begin to view reading code as a skill that you should practice like you practiced reading in grade school, and here’s how to start. One of the first things that most programmers complain about is the ‘style’ of other people’s code. We’re all trained in a different way - you might have learned it from an online boot camp, you might be self taught, or you might have had a professor who was hell bent on teaching algorithms in his self-designed programming language and gave the bird to everything else (that might just be a personal experience, idk). Trying to just get an excellent sense of what’s going on, what the programmer’s mental model of the code was just from looking at it is insufficient. It’s more like sight reading for piano than actual reading because it exists in a different dimension than regular books. It’s functional and takes in different objects (i.e. datatypes). Therefore, you need different tools. ","First and foremost - <b>RUN THE CODE!</b>. Never, ever just look at it blankly and assume you don’t understand how it works. RUN IT! Download it to your local machine and see what it does! You’d be surprised how much information you can get out of something when the initial build doesn’t even work. I’ve seen too many of my students and fellow researchers struggle to get something to work that they download from somewhere else and never launched the original code in the first place. One of the main complaints from my Master’s program was how all of our professor-designed programs took ages to just get up and running. It’s easier than you think to get around this hurdle, just run the program and see what you’re missing. It’s usually just as simple as installing the right packages. ",Skills,Programming,Code,Practice,https://spin.atomicobject.com/2017/06/01/how-to-read-code/,How to Read Code (Eight Things to Remember),https://medium.com/@mrlauriegray/the-way-to-read-other-peoples-code-without-ending-up-crying-dd71fee6d005,The Way to Read Other People’s Code Without Ending Up Crying,https://skorks.com/2010/05/why-i-love-reading-other-peoples-code-and-you-should-too/,Why I Love Reading Other People’s Code and You Should Too ,"One thing I’m always shocked at is how new programmers aren’t taught how to debug their own programs. Like parallelization, Linux, working with packages, communicating their results, and developing comprehensive tests - debugging is just one of those concepts that CS faculty just believes students can get on their own. Here’s how to actually go about understanding code by debugging. Step One - find an IDE (Integrated Development Environment) that you can use to hold the code. Nobody really covers this in introductory programming classes - a lot of online tutorials I’ve found insist on using the Python native shell and script (which is confusing if you don’t understand how computer architectures work) or their own unique environment which just compounds the problem by making people reliant on using that system until they can break out on their own (yes I view Udacity and Treehouse’s in-house programming environments as jails, despite their excellent tutorials). If you’re working with Python, I recommend using PyCharm. Their debugger is pretty easy to understand once you get everything working (I cover starting a project in PyCharm in <b>another post</b>, it can get a bit wonky - which is probably why so many beginners don’t use IDEs for their projects). The main concept to understand when you’re starting out are breakpoints. You usually place them wherever you think the error is - but you can also just use it to step through the code and see what everything is. It’s extremely helpful when you’re given a piece of foreign code that you want to tool through to get a sense of what’s going on. Please see my whole post on debugging with PyCharm <b>here</b>, because the whole debugging process is a bit beyond the scope of this post. The point of using debugging is to get a sense of what all the various inputs and outputs of a piece of code are. Once you get that sense, you can do anything with the code. ","Moving on, here are some other tips and tricks I’ve come up with in my last few years as a developer that might be useful towards understanding others’ code. First, read through the documentation. Documentation can be its own beast to work with - which is why I cover it in another post <b>here</b>, but a quick skim of what the author’s talking about can be very useful towards getting your mind where it needs to be to understand the code. Let this be a note to developers in general - we all find it so obnoxious when we hit code that’s undocumented, yet we find documenting a pain. We can’t have it both ways, so <b>document your damn code!</b> Second, start with the <b>main function</b>. I know I mentioned that everybody gets taught differently, but one thing that seems fairly constant among most professionals is the use of a main function to launch the code when it’s run from the command line (i.e. using it on terminal or part of a pipeline). This is one very essential way that code is different from books. In books, you’ll read from left to right, top to bottom (unless you’re reading and writing in a Middle Eastern language like Arabic or Hebrew, in which case it’s right to left). You should be reading code from bottom up (paying attention to which packages are imported at the top while you do it). All of the essential building blocks of the program will be described in the main function. You’ll be able to divide the different parts of the program based on reading through the lines of main. Once those parts are divided, you can begin splitting the code into smaller and more manageable pieces. That’s the whole theory of understanding code - it’s all about breaking everything down into smaller and smaller pieces until you can manage it. The main provides the best guide to the code - <b>think of it like a table of contents</b>. ","Once you’ve identified this ‘table of contents,’ split up the code into helper functions and classes. Classes are the larger of the two, so I’ll start with those. Identify what classes it inherits from (if it even does inherit - a small enough codebase won’t have a lot of overlap). Identify the methods and attributes of the class. The attributes will be variables that every instance of the class gets when it’s called. I find most initial attempts to describe classes a bit frustrating - they limit it to simple objects like characters in a video game (i.e. every character has X quality), which I haven’t found all that useful to transfer over to advanced coding. I go into a better description of how to understand classes <b>here</b>, but for now, just understand that looking into classes and dividing up the attributes and methods is the best way to tackle them. Methods are just the helper functions that are unique to the instance of the class. Second, the helper functions are the additional processes outside of any classes that are used by the main function to do something to whatever the inputs were (I’ve found they’re usually used in preprocessing, but they’re flexible enough to apply to anything). ","Anytime you hit something you don’t understand - what datatype a variable is, where a certain method came from, etc. - use the tools that IDE gives you to clue you in. Find every instance of the method and see where it was defined. Use the IDE to strip the code bare. No part of the code is hidden to you - proper use of an IDE will reveal everything. This is a very short introduction to reading and understanding code. It gets difficult, especially when you start working with others’ packages that will be thousands of lines of code. This is why programmers get paid a lot of money - it’s a hard job and can get complicated. The good news is that if you regard reading code as a skill instead of a given, like most CS professors seem to believe, you can stay positive and motivated to continue learning. This is a difficult thing, and don’t let anybody convince you that you “need to have a certain mind for programming,” or “you’re just not good at it.” Anybody can be a programmer, they just need to practice. ",If you have your own strategies or inputs on how to read code - let me know in the comments below. I can always improve these posts. ,,,,,,Something
2,The Chicken and the Egg of Named Entity Recognition ,Using algorithms to identify and pull out certain elements of text can be easy - what’s difficult is getting the data right.,/index.html,Noah Caldwell-Gatsos,mailto:ncaldwellgatsos@gmail.com,My Email,8/13/19,5:25PM CST ,/projects/ai/images/bp2front.png,A rendering of SpaCy’s Named Entity Parser in HTML ,SpaCy,"In this post, I want to discuss the differences between 1) rule-based systems, 2) supervised automata, and 3) hybrid active semi-supervised automata, and 4) fully unsupervised methods. The main issue with NER as it stands is that to get the data to train a model, you need to have the model already trained. ","Rule-based systems are very similar to find & replace algorithms with a foundation in regular expressions. Regular expressions are the use of literal and special alphabetic characters that can be used to find specific patterns in a text-corpora. The patterns are identified using ‘rules’ (i.e. every instance ‘X’ to be labeled follows X’s specific pattern) Early automated annotation systems often were rule-based, but this poses several problems for domain-specific NER tasks, or anything beyond analyzing large bodies of ‘low-intensity text’. Low-intensity in this case meaning news articles, excerpts of novels, and other bodies of literature that use common language. Rule-based systems are brittle and often can’t capture any kind of nuance or diversity in the text that they encounter. They also require a lot of manual effort to construct, domain-specific expertise, and a decent amount of knowledge of RegEx, which can arguably become quite complex with certain tasks. Rule-based systems have an advantage in their efficiency and speed - especially for smaller tasks like find and replace, but when they’re asked more complex tasks, they aren’t up to the job. ","The next system of applied NER is supervised automata, which comprises the majority of mainstream, off-the-shelf NER tools online. SpaCy is an excellent example of this. To build a supervised automated system, a corpus of the desired domain is manually annotated with named entities with the types you wish to identify. These annotations - identifying the examples of the named entities that you want to locate and extract - are used to train a model to recognize these patterns. Entities can be locations, expressions of dates or times, or anything that can be classified. Practitioners of supervised automata typically use pre-annotated general datasets that can be obtained from machine learning hubs like Kaggle. This reliance on general datasets like the ones provided by services such as these is a big part of why the widespread use of supervised automata is limited to general knowledge extraction or like I mentioned in the last section - low-intensity writing. Supervised automata typically begin by using the dataset to train a ‘memorizer’ through ML packages like sklearn and evaluate them using metrics like 5-fold cross-validation. Scores like precision, recall, and the f1-score are used to determine the model’s success and are common in NLP tasks. Machine learning is used to improve recall, which is the system’s ability to adapt to new, previously unseen information. While formerly a very difficult task, packages like sklearn make the step of making the data usable by a ML approach by converting the data to simple feature vectors (i.e. numbers) easy. Creating a feature map (i.e. a matrix of numbers) is relatively simple by combining sklearn’s RandomForestClassifier, Label Encoder, Memory Tagger, Pipeline, and NumPy arrays. Results from using sklearn can be further improved with more sophisticated methods like conditional random fields, neural networks, long-short term memory networks, character embeddings, residual long-short term memory systems, ELMo, and BERT (<- yes, those are names of Sesame Street characters, it’s an inside joke among NLP engineers). ","Manually annotating large corpora to use and create training data from is expensive, requires new annotations every time a new named entity is defined or a new field-specific body wants to be covered. Supervised automata would actually be the perfect approach if you could get your hands on perfectly annotated data every time you wanted to work with named entities. Unfortunately - and especially in high-paid, highly technical fields like mathematics, clinical studies, law, and less data driven humanities - it’s really difficult to not only find someone willing to annotated that much information at low cost, but also to even determine what is worth annotating in the first place. Some scientists have developed workarounds for this - figuring out proxy information to substitute for specific labels is the main role of data engineers. Existing dictionaries or repositories of known named entities to match in text with mechanisms for unsupervised disambiguation or contextual evidence is a significant approach. However, these can largely only apply to the low-intensity corpora that I mentioned above. A significant reason why these methods don’t work for high-intensity corpora is because named entities in these fields (clinical studies, law) have linguistic variations. There are several ways to refer to the same named entity. This makes it difficult for semi-supervised automata to work well. Second, high-intensity named entities are often multi-token terms with nested structures that include several other entities inside them. This can make determining boundaries of these structures a challenge in and of itself. ","That leads me to #4 - completely unsupervised automata based on computational pattern recognition. Current systems are typically derived from language formatting hypotheses - one such example being Zhang and Elhadad 2013, which observed that named entities in the same class tend to have similar vocabulary and occur in similar contexts [1]. Similar systems typically start with a resource describing all of the possible terms that can a named entity can comprise of - like a clinical UMLS (Unified Medical Language System), the terms are then matched wherever they occur as noun phrase chunks in a corpus. A signature is created for each seed term in the form of vector representation based on the inverse document frequencies (IDF) of the words occurring within the term and the words occurring in the contexts in which the term occurs in the corpus. The context of a term occurrence is defined as the previous and the next two words. A signature of the target entity class is then computed by averaging the signature of all its seed terms. During testing, the method first computes signature for a candidate entity and then computes its cosine similarity with the signatures of all the entity classes. The candidate entity is assigned the entity class with which is has the highest cosine similarity, as long as that similarity is above a predetermined threshold. This is but one method of creating an unsupervised NER - combining a database of potential words, vectorizing them based on their IDF given a certain context, deriving their signature, and assigning a label based on cosine similarity measures. Improvements to this method can be added through machine learning applications applied to an automatically annotated corpus, allowing the system to recognize named entities that can occur in varying contexts. Full parsing of the vocabulary can also improve the potential matches. ",NER,Big Data,Code,Machine Learning,https://www.sciencedirect.com/science/article/pii/S2352914818301965#bib20,Learning for clinical named entity recognition without manual annotations,https://www.sciencedirect.com/science/article/pii/S1532046413001196,Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts,https://towardsdatascience.com/highest-rated-ml-projects-on-github-694486293512,Highest Rated ML Projects on Github,"The final form of named entity recognition is active learning semi-supervised models - a way to benefit from the extensive literature and proven success of supervised models, but avoiding the ‘chicken and egg’ situation where in order to get large amounts of the data required to train the models you need to have developed the model in the first place. Examples of this kind of named entity recognition includes training using automatically generated annotations, or focused singularly on relation extraction. The active learning component exists in tools like SpaCy’s Prodigy, a way of reducing the manual annotation effort (despite still requiring some kind manual annotation to begin with). The learning process grows the corpus of data interactively and wisely by making the system ask for the most helpful examples that the human should annotate or reduces the annotation process to a simple binary classifier (i.e does this example match or not). Other methods of reducing the annotation burden include pre-annotation, where a system first automatically annotates a corpus that a human corrects. Provided that the pre-annotated corpus is of high-quality, this can save a large amount of effort. ","The unsupervised learning approach shows the most promise. This method could greatly reduce the manual effort and cost of building high-intensity NER models, which is really the only worthwhile application of NER for information gathering purposes (low-intensity NER is good for later downstream tasks). Unsupervised models can also be used to strengthen other semi-supervised models by providing more annotations automatically (thus, solving the chicken and the egg).  ",,,,,,,,,Something
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,  ,,,