,title,subtitle,aut_href,author,ema_href,email,date,time,image_path,caption,source,one_p,two_p,three_p,four_p,five_p,tag1,tag2,tag3,tag4,link_n1,next1,link_n2,next2,link_n3,next3,extra1,extra2,extra3,extra4,extra5,extra6,extra7,extra8,extra9,extra10,metatitle 
1,Importance of Reading Code and Technical Papers,Why Reading Other People’s Code and Intense Technical Papers is the Key to your Success as a Developer,/index.html,Noah Caldwell-Gatsos,mailto:ncaldwellgatsos@gmail.com,My Email,8/1/19,12:33PM CST ,/projects/ai/images/debugger.jpg,"An Example of a PyCharm Debugger, It’s a Lot Simpler Than it Looks ",Google Images ,"I get asked a lot how I can find reading fun - I’ve gotten caught at social gatherings flipping through pages of a book (I know, I’m a party animal) and asked if I genuinely enjoy it. In all honesty, it’s astounding to me that less people enjoy reading, since it’s something that I rarely have to think about while I’m doing it. It’s surprising that most people find it a chore. Something that that’s recently given me a fair amount of insight towards others’ opinions on my reading habits is developing my own skills as a programmer. One of the worst things I have to do everyday is read other people’s code. It’s boring, I often get lost from one line to the next, I don’t know what everything means, etc. In other words - it’s probably exactly what the majority of people who remark that my reading habits are strange are thinking when they tackle lengthy online articles or novels. There’s obviously some differences - code is semantically different from plain English - but making that comparison has led to me to believe that there’s a way to make reading code and rough technical papers more manageable. ","What’s that way? The same way that I grew to love reading everyday in a way that other people haven’t: practice. Reading’s an under-appreciated skill. I think it would surprise folks to learn that widespread literacy is a relatively recent phenomenon. World illiteracy didn’t <a href=“https://unesdoc.unesco.org/ark:/48223/pf0000247563”>start rapidly declining </a> until the 1970s. It has historically been a mark of the elite and well-educated (i.e. those with access to resources), but in modern times it’s relatively commonplace in the Western world. You’d be surprised to find someone who grew up speaking English fluently with full access to decent public education and used to surfing the web who wasn’t able to read above a fifth grade level. The age we live in is termed the ‘Information Age’. The point I want to make is that people have begun to see reading as a given, instead of a skill - which I worry is hindering a lot of people’s success, not only in actually reading, but in jobs and skills that require it peripherally, like coding. Another example I can think of is giving up on sight-reading in my piano lessons when it got hard. I was so used to the ease of regular reading that appreciating that sight reading was a different skill entirely was a foreign concept to me. I gave up on it because it wasn’t easy. I don’t want to do the same with code. ","Before I explain how to improve your code, it’s essential that I clarify <b>why</b> it’s important to know how to read code well. As of August 2019, I’m the solo leader on a startup programming project - I built our system from scratch and I only had to clarify inputs and outputs for the other team members working on our  system. I <b>still have to read other people’s code</b> all the time. I have to be able to quickly and fluently understand the inner workings of a variety of different programming packages, GitHub open source hacks, and workarounds developed by other folks in the community everyday just to get my stuff to function properly. If you’re a professional software engineer working with a team and a deadline, this is even more essential. A typical software development team has a variety of programmers from all kinds of backgrounds. You’re going to have to sort through their code, understand it just as well as something of your own, and actually produce results from that understanding. All probably on a deadline. It can’t be something that you view as a chore, or something that’s a pain to get done. It has to be something you’re well versed in and can get excited to do. In other words, it has to be as exciting as reading a semi-decent book. I’m not saying get caught at social gatherings reading code, but try and get your skills to a point where reading code becomes more of a fun exercise than something you wouldn’t touch with a five foot pole. ","That begins my main premise: begin to view reading code as a skill that you should practice like you practiced reading in grade school, and here’s how to start. One of the first things that most programmers complain about is the ‘style’ of other people’s code. We’re all trained in a different way - you might have learned it from an online boot camp, you might be self taught, or you might have had a professor who was hell bent on teaching algorithms in his self-designed programming language and gave the bird to everything else (that might just be a personal experience, idk). Trying to just get an excellent sense of what’s going on, what the programmer’s mental model of the code was just from looking at it is insufficient. It’s more like sight reading for piano than actual reading because it exists in a different dimension than regular books. It’s functional and takes in different objects (i.e. datatypes). Therefore, you need different tools. ","First and foremost - <b>RUN THE CODE!</b>. Never, ever just look at it blankly and assume you don’t understand how it works. RUN IT! Download it to your local machine and see what it does! You’d be surprised how much information you can get out of something when the initial build doesn’t even work. I’ve seen too many of my students and fellow researchers struggle to get something to work that they download from somewhere else and never launched the original code in the first place. One of the main complaints from my Master’s program was how all of our professor-designed programs took ages to just get up and running. It’s easier than you think to get around this hurdle, just run the program and see what you’re missing. It’s usually just as simple as installing the right packages. ",Skills,Programming,Code,Practice,https://spin.atomicobject.com/2017/06/01/how-to-read-code/,How to Read Code (Eight Things to Remember),https://medium.com/@mrlauriegray/the-way-to-read-other-peoples-code-without-ending-up-crying-dd71fee6d005,The Way to Read Other People’s Code Without Ending Up Crying,https://skorks.com/2010/05/why-i-love-reading-other-peoples-code-and-you-should-too/,Why I Love Reading Other People’s Code and You Should Too ,"One thing I’m always shocked at is how new programmers aren’t taught how to debug their own programs. Like parallelization, Linux, working with packages, communicating their results, and developing comprehensive tests - debugging is just one of those concepts that CS faculty just believes students can get on their own. Here’s how to actually go about understanding code by debugging. Step One - find an IDE (Integrated Development Environment) that you can use to hold the code. Nobody really covers this in introductory programming classes - a lot of online tutorials I’ve found insist on using the Python native shell and script (which is confusing if you don’t understand how computer architectures work) or their own unique environment which just compounds the problem by making people reliant on using that system until they can break out on their own (yes I view Udacity and Treehouse’s in-house programming environments as jails, despite their excellent tutorials). If you’re working with Python, I recommend using PyCharm. Their debugger is pretty easy to understand once you get everything working (I cover starting a project in PyCharm in <b>another post</b>, it can get a bit wonky - which is probably why so many beginners don’t use IDEs for their projects). The main concept to understand when you’re starting out are breakpoints. You usually place them wherever you think the error is - but you can also just use it to step through the code and see what everything is. It’s extremely helpful when you’re given a piece of foreign code that you want to tool through to get a sense of what’s going on. Please see my whole post on debugging with PyCharm <b>here</b>, because the whole debugging process is a bit beyond the scope of this post. The point of using debugging is to get a sense of what all the various inputs and outputs of a piece of code are. Once you get that sense, you can do anything with the code. ","Moving on, here are some other tips and tricks I’ve come up with in my last few years as a developer that might be useful towards understanding others’ code. First, read through the documentation. Documentation can be its own beast to work with - which is why I cover it in another post <b>here</b>, but a quick skim of what the author’s talking about can be very useful towards getting your mind where it needs to be to understand the code. Let this be a note to developers in general - we all find it so obnoxious when we hit code that’s undocumented, yet we find documenting a pain. We can’t have it both ways, so <b>document your damn code!</b> Second, start with the <b>main function</b>. I know I mentioned that everybody gets taught differently, but one thing that seems fairly constant among most professionals is the use of a main function to launch the code when it’s run from the command line (i.e. using it on terminal or part of a pipeline). This is one very essential way that code is different from books. In books, you’ll read from left to right, top to bottom (unless you’re reading and writing in a Middle Eastern language like Arabic or Hebrew, in which case it’s right to left). You should be reading code from bottom up (paying attention to which packages are imported at the top while you do it). All of the essential building blocks of the program will be described in the main function. You’ll be able to divide the different parts of the program based on reading through the lines of main. Once those parts are divided, you can begin splitting the code into smaller and more manageable pieces. That’s the whole theory of understanding code - it’s all about breaking everything down into smaller and smaller pieces until you can manage it. The main provides the best guide to the code - <b>think of it like a table of contents</b>. ","Once you’ve identified this ‘table of contents,’ split up the code into helper functions and classes. Classes are the larger of the two, so I’ll start with those. Identify what classes it inherits from (if it even does inherit - a small enough codebase won’t have a lot of overlap). Identify the methods and attributes of the class. The attributes will be variables that every instance of the class gets when it’s called. I find most initial attempts to describe classes a bit frustrating - they limit it to simple objects like characters in a video game (i.e. every character has X quality), which I haven’t found all that useful to transfer over to advanced coding. I go into a better description of how to understand classes <b>here</b>, but for now, just understand that looking into classes and dividing up the attributes and methods is the best way to tackle them. Methods are just the helper functions that are unique to the instance of the class. Second, the helper functions are the additional processes outside of any classes that are used by the main function to do something to whatever the inputs were (I’ve found they’re usually used in preprocessing, but they’re flexible enough to apply to anything). ","Anytime you hit something you don’t understand - what datatype a variable is, where a certain method came from, etc. - use the tools that IDE gives you to clue you in. Find every instance of the method and see where it was defined. Use the IDE to strip the code bare. No part of the code is hidden to you - proper use of an IDE will reveal everything. This is a very short introduction to reading and understanding code. It gets difficult, especially when you start working with others’ packages that will be thousands of lines of code. This is why programmers get paid a lot of money - it’s a hard job and can get complicated. The good news is that if you regard reading code as a skill instead of a given, like most CS professors seem to believe, you can stay positive and motivated to continue learning. This is a difficult thing, and don’t let anybody convince you that you “need to have a certain mind for programming,” or “you’re just not good at it.” Anybody can be a programmer, they just need to practice. ",If you have your own strategies or inputs on how to read code - let me know in the comments below. I can always improve these posts. ,,,,,,Something
2,The Chicken and the Egg of Named Entity Recognition ,Using algorithms to identify and pull out certain elements of text can be easy - what’s difficult is getting the data right.,/index.html,Noah Caldwell-Gatsos,mailto:ncaldwellgatsos@gmail.com,My Email,8/3/19,5:25PM CST ,/projects/ai/images/bp2front.png,A rendering of SpaCy’s Named Entity Parser in HTML ,SpaCy,"In this post, I want to discuss the differences between 1) rule-based systems, 2) supervised automata, and 3) hybrid active semi-supervised automata, and 4) fully unsupervised methods. The main issue with NER as it stands is that to get the data to train a model, you need to have the model already trained. ","Rule-based systems are very similar to find & replace algorithms with a foundation in regular expressions. Regular expressions are the use of literal and special alphabetic characters that can be used to find specific patterns in a text-corpora. The patterns are identified using ‘rules’ (i.e. every instance ‘X’ to be labeled follows X’s specific pattern) Early automated annotation systems often were rule-based, but this poses several problems for domain-specific NER tasks, or anything beyond analyzing large bodies of ‘low-intensity text’. Low-intensity in this case meaning news articles, excerpts of novels, and other bodies of literature that use common language. Rule-based systems are brittle and often can’t capture any kind of nuance or diversity in the text that they encounter. They also require a lot of manual effort to construct, domain-specific expertise, and a decent amount of knowledge of RegEx, which can arguably become quite complex with certain tasks. Rule-based systems have an advantage in their efficiency and speed - especially for smaller tasks like find and replace, but when they’re asked more complex tasks, they aren’t up to the job. ","The next system of applied NER is supervised automata, which comprises the majority of mainstream, off-the-shelf NER tools online. SpaCy is an excellent example of this. To build a supervised automated system, a corpus of the desired domain is manually annotated with named entities with the types you wish to identify. These annotations - identifying the examples of the named entities that you want to locate and extract - are used to train a model to recognize these patterns. Entities can be locations, expressions of dates or times, or anything that can be classified. Practitioners of supervised automata typically use pre-annotated general datasets that can be obtained from machine learning hubs like Kaggle. This reliance on general datasets like the ones provided by services such as these is a big part of why the widespread use of supervised automata is limited to general knowledge extraction or like I mentioned in the last section - low-intensity writing. Supervised automata typically begin by using the dataset to train a ‘memorizer’ through ML packages like sklearn and evaluate them using metrics like 5-fold cross-validation. Scores like precision, recall, and the f1-score are used to determine the model’s success and are common in NLP tasks. Machine learning is used to improve recall, which is the system’s ability to adapt to new, previously unseen information. While formerly a very difficult task, packages like sklearn make the step of making the data usable by a ML approach by converting the data to simple feature vectors (i.e. numbers) easy. Creating a feature map (i.e. a matrix of numbers) is relatively simple by combining sklearn’s RandomForestClassifier, Label Encoder, Memory Tagger, Pipeline, and NumPy arrays. Results from using sklearn can be further improved with more sophisticated methods like conditional random fields, neural networks, long-short term memory networks, character embeddings, residual long-short term memory systems, ELMo, and BERT (<- yes, those are names of Sesame Street characters, it’s an inside joke among NLP engineers). ","Manually annotating large corpora to use and create training data from is expensive, requires new annotations every time a new named entity is defined or a new field-specific body wants to be covered. Supervised automata would actually be the perfect approach if you could get your hands on perfectly annotated data every time you wanted to work with named entities. Unfortunately - and especially in high-paid, highly technical fields like mathematics, clinical studies, law, and less data driven humanities - it’s really difficult to not only find someone willing to annotated that much information at low cost, but also to even determine what is worth annotating in the first place. Some scientists have developed workarounds for this - figuring out proxy information to substitute for specific labels is the main role of data engineers. Existing dictionaries or repositories of known named entities to match in text with mechanisms for unsupervised disambiguation or contextual evidence is a significant approach. However, these can largely only apply to the low-intensity corpora that I mentioned above. A significant reason why these methods don’t work for high-intensity corpora is because named entities in these fields (clinical studies, law) have linguistic variations. There are several ways to refer to the same named entity. This makes it difficult for semi-supervised automata to work well. Second, high-intensity named entities are often multi-token terms with nested structures that include several other entities inside them. This can make determining boundaries of these structures a challenge in and of itself. ","That leads me to #4 - completely unsupervised automata based on computational pattern recognition. Current systems are typically derived from language formatting hypotheses - one such example being Zhang and Elhadad 2013, which observed that named entities in the same class tend to have similar vocabulary and occur in similar contexts [1]. Similar systems typically start with a resource describing all of the possible terms that can a named entity can comprise of - like a clinical UMLS (Unified Medical Language System), the terms are then matched wherever they occur as noun phrase chunks in a corpus. A signature is created for each seed term in the form of vector representation based on the inverse document frequencies (IDF) of the words occurring within the term and the words occurring in the contexts in which the term occurs in the corpus. The context of a term occurrence is defined as the previous and the next two words. A signature of the target entity class is then computed by averaging the signature of all its seed terms. During testing, the method first computes signature for a candidate entity and then computes its cosine similarity with the signatures of all the entity classes. The candidate entity is assigned the entity class with which is has the highest cosine similarity, as long as that similarity is above a predetermined threshold. This is but one method of creating an unsupervised NER - combining a database of potential words, vectorizing them based on their IDF given a certain context, deriving their signature, and assigning a label based on cosine similarity measures. Improvements to this method can be added through machine learning applications applied to an automatically annotated corpus, allowing the system to recognize named entities that can occur in varying contexts. Full parsing of the vocabulary can also improve the potential matches. ",NER,Big Data,Code,Machine Learning,https://www.sciencedirect.com/science/article/pii/S2352914818301965#bib20,Learning for clinical named entity recognition without manual annotations,https://www.sciencedirect.com/science/article/pii/S1532046413001196,Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts,https://towardsdatascience.com/highest-rated-ml-projects-on-github-694486293512,Highest Rated ML Projects on Github,"The final form of named entity recognition is active learning semi-supervised models - a way to benefit from the extensive literature and proven success of supervised models, but avoiding the ‘chicken and egg’ situation where in order to get large amounts of the data required to train the models you need to have developed the model in the first place. Examples of this kind of named entity recognition includes training using automatically generated annotations, or focused singularly on relation extraction. The active learning component exists in tools like SpaCy’s Prodigy, a way of reducing the manual annotation effort (despite still requiring some kind manual annotation to begin with). The learning process grows the corpus of data interactively and wisely by making the system ask for the most helpful examples that the human should annotate or reduces the annotation process to a simple binary classifier (i.e does this example match or not). Other methods of reducing the annotation burden include pre-annotation, where a system first automatically annotates a corpus that a human corrects. Provided that the pre-annotated corpus is of high-quality, this can save a large amount of effort. ","The unsupervised learning approach shows the most promise. This method could greatly reduce the manual effort and cost of building high-intensity NER models, which is really the only worthwhile application of NER for information gathering purposes (low-intensity NER is good for later downstream tasks). Unsupervised models can also be used to strengthen other semi-supervised models by providing more annotations automatically (thus, solving the chicken and the egg).  ",,,,,,,,,Something
3,How to Rock a Cognitive AI Interview ,Go beyond the easy buzzwords - here’s what you need to get an in-depth understanding of what’s going on with cognitive systems.,/index.html,Noah Caldwell-Gatsos,mailto:ncaldwellgatsos@gmail.com,My Email,8/6/19,4:40PM CST ,/projects/ai/images/bp3front.jpg,How AI and Cognitive Science Meet ,Google Images,"It seems like everybody and their mother has some new AI trend they want to make money off of these days. Jargon-filled phrases fly around like Natural Language Processing, Reinforcement Learning, Neural Nets, Big Data, and Machine Learning. It can be easy to get lost, especially because a lot of them are just variations on the same thing. I recently had a friend go through the application process for a company that specialized in ‘cognitive AI’. Here’s the breakdown of what Cognitive AI is, how to talk about it, and the right questions to ask of an employer when you’re in the interview. ","AI’s major short-term private-sector value is best suited towards helping companies minimize the shortcomings of their current workload through the automatic ability to act on near-perfect information (i.e. observations from big data) without much human input. Most companies that focus on resource extraction or manufacturing suffer from over and under-production. If they have too much of a certain product, they can’t make money off it. If they have too little of a product, that’s lost revenue. Most companies also can’t quickly shift production levels from high to low production - these are decisions that are made far in advance off of what production managers believe will be the long-term trends. It’s the best strategy that these companies have - but could it be improved? Sounds like a perfect recipe for AI to fix.","The main thing that you’ll want to talk about in any AI-related interview is <b>data</b>. Data collection, organization, housing, and processing are the four main attributes that you’ll be interested it. It’s really unfortunate that most companies are reactive to big data concerns than proactive - it’s often an afterthought - but that’s perfect for you to take advantage of. Most companies ignore these concerns because they don’t understand them, or they’ve hired the machine learning engineers thinking that they’ll be able to adapt (bad strategy). Data is essential to understanding the foundations of what your product is. Not having a thorough understanding of the data is like an architect not knowing what their building materials are. Ask your prospective employer what their data collection strategy is. How is it maintained? How is it processed for use in machine learning algorithms? These are big questions, and if they don’t know the right answer - be prepared, and use it as an opportunity to show off. ","Now onto why you clicked on this article. How does Deep Reinforcement Learning (otherwise known as Cognitive AI) actually work? What do you need to know to sound knowledgeable about it?  Let’s start with what it’s used for. Using DRL, you can design automatic control functionality for robots, create theoretical worlds and fill them with potential factors to model future situations, allow machines to make decisions based on the world around them, and master game AI. The six main areas of DRL are <ol><li>Automatic Control and Imitation Learning</li><li>Reinforcement Learning</li><li>Deep Feedforward Networks</li><li>Reinforcement Learning in Large State Spaces</li><li>Policy Gradients and the Q Function</li><li>Reinforcement in Continuous Action Spaces</li></ol>. Each of these six subjects are vast enough to merit their own post (or several) on their own. You can read up on them anywhere online if you’ve got the time. We want to focus on how to talk about each of them. If you actually want to get into the technical aspects of Deep Reinforcement Learning, I recommend that you have prior experience with Deep Learning, Machine Learning, and a good understanding of Python programming with standard libraries like Numpy, Pandas, and Matplotlib.","If you’re feeling a bit lost - don’t panic! Everybody does when they get to this stuff. A good way to describe computer science is like a single tool that everybody and their brother managed to customize in their own unique way, and is desperate to get you to learn it. Nobody knows all this stuff at once, and even the most experienced machine learning engineers refer to the basics all the time. What’s best is to generalize - and let’s start generalizing with DRL. A good concept to grasp is the idea of <b>Dynamic Systems</b>. Dynamic systems are labeled as such because they’re constantly changing. That’s why they’re useful - they can take in constantly changing data and do stuff with it! The second general concept is <b>memory</b>. Memory is important because that’s all the information that your system can use to make decisions. Now - machines handle memory a bit different than you or I do. It’s not like short-term memory vs. long-term for us. Instead, the metric that machines use as a data source is limited vs. unlimited memory. Each has differing reasons for its use. You want a dynamic system with limited memory if you don’t want the information from a huge timeframe to interfere with the actions of the system (i.e. last summer had a record-breaking heat but you don’t want your system to think that <b>every</b> summer is going to be like that!). Sometimes you’ll want a dynamic system with unlimited memory to handle long stretches of time and produce outputs. It all depends on your use case - but remember! Computers have their limits too. If you try and store everything, you’ll waste computational resources. The reason machine learning engineers get paid so much is because they have to determine which features are worth keeping in memory. Google Cloud gets expensive! Why did I introduce those two concepts over all the others? Dynamic systems really emphasize the point of deep reinforcement learning,  which is to use the stimuli around the machine to get it to make a decision. You want the machine to take in everything around it, analyze all the data, and make a decision (i.e. the output). ",ML,Interviews,ReinforcementLearning,CognitiveAI,https://venturebeat.com/2019/07/28/deep-learning-is-about-to-get-easier-and-more-widespread/amp/,Deep Learning is about to get easier and more widespread ,https://www.forbes.com/sites/bernardmarr/2016/03/23/what-everyone-should-know-about-cognitive-computing/#3c7db52f5088,What Everyone Should Know About Cognitive Computing,http://www.r2d3.us/visual-intro-to-machine-learning-part-1/?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BR9B4rGtnTmSrw4DAoezDjQ%3D%3D,A visual introduction to machine learning,"The next big general concepts are predictive learning problems. There are two main predictive learning problems - <b>regression</b> and <b>classification</b>. These are terms that’ll be familiar to any statistician or data scientist. How would you go about solving a regression problem? Take ‘predicting the share price of a company about to go public.’ You’ll want to get a training dataset with known share prices, and try and <b>design features that are thought to be relevant to your goal</b>. This can be things like the company’s current revenue (i.e. the higher the revenue, the higher the stock price). In order to connect the potential share price to the revenue, you’ll use a linear model or <i>regression line</i>. These kinds of decisions are what DRL is ideal for. Using ML for classification is similar - but instead of predicting the future of an output, you’re trying to predict a class (or a label). This kind of problem solving can be found in object recognition, where the machine learning model needs to distinguish an object from its surroundings and identify it. Mathematically, the way you do classification is by finding a hyperplane (i.e. a separating line) that separates the two classes of data from a training set as best as possible. ","I’m only going to cover one more general concept before I go into some summarizations and ideal questions to ask with broad answers that you won’t have to know everything to make it look like you’ve got this in the bag. The final concept is feature design - figuring out those defining characteristics of a given dataset that allow for optimal learning. <b>The more you understand the process of generating the data that you have, the better you can design features yourself, or even better, teach the computer to design them itself.</b> One of the fun end goals of machine learning is to develop adequate tools to understand our own universe - how to find patterns in otherwise arbitrary data and how to identify the best ones to use for significant observations. ","One thing you’ll probably want to discuss is how machine learning actually works. Above, I just listed the beginning tools to understand ML, but you probably want to know how they come together and actually solve a problem - right? Every machine learning problem eventually reduces to just <b>finding the minimum of an associated cost function</b>. If that sounds complex, it honestly is! A good way to think about this is how the brain has ridges, kind of like a canyon on a super small scale. That’s where a funny thing happens - we’ve figured out that if we can shape the canyon right, the answer we want is always at the bottom! I can practically hear all the mathematicians groaning collectively at that explanation, but it’s honestly a really good preliminary way to understand the ‘magic’ that goes on in machine learning systems. The real answer is more similar to the exercises you had to do in calculus where you were given a missing value in a series of linear equations (i.e. a matrix) and told to find the missing value. In this case, the missing value is the bottom of the canyon, and the canyon is the representation of the linear equations! If you’re still confused about the abstraction, don’t worry - it’s a really weird concept for the human brain to grasp. ","Want some good news? It’s really easy to talk about machine learning in an interview if it’s more theoretical than technical. A good way to stick to a script is to be honest about what’s exciting about these kinds of technological breakthroughs - the potential of machine learning is really, really cool! That whole concept of using computers to <b>find and make use of patterns in an otherwise chaotic universe</b> should be fascinating for everyone. You’re pretty much designing a virtual ‘manager’ with perfect information for a product if you’re working with this in industrial production. What’s better than that? I covered the idea of features above - you know what your features are? They’re the manufacturing information - the number of people working, the acres of equipment in operation, the amount of energy used, the inputs and outputs. They’re the situational awareness - the knowledge of the market, the knowledge of the surrounding area, the knowledge of what allows the inputs and outputs to come about. And both of those are combined to allow for perfect coordination. It’s all one brain pretty much analyzing everything and making decisions. Cool right? If you want the key to talking about machine learning, just stick to what’s positive about the whole process. The possibilities are endless. ","Definitely inquire about the following three areas when you go into your interview. Depending on how confident you are in your knowledge of each area, alter and edit the questions to suit your needs. <ol><li>Focus on data collection. Ask how statistically significant features are engineered by the company. Ask how those features are record and then processed to be of optimal use for the company’s purposes. Not having data in the right format is the biggest problem. The single best question you can ask is <b>how they label their data points</b>. That will immediately clue your boss in that you know what the main issues are in machine learning. Unlabeled data is the bane of all machine learning engineers.</li><li>Ask about their use of cloud storage. Do they have adequate staff on hand to be able to deal with the complexities of managing a cloud data platform and pipeline to their services? Just having your intern attempt to launch a model on a GPU is not enough.</li><li>Ask whether their dynamic systems involved long-term memory or short-term memory, or a mixture of both. That will clue you in as to whether or not the company’s thought intensively about their data collection practices and how they make use of them. A good follow up is what situations would you use both types of memory for.</li></ol>",Shoot me an email if you’d like more potential questions or things to know about deep reinforcement learning. It’s a growing field and it’s hard to find easy to grasp answers online. Check out the read next options I have below for more information. I didn’t even cover implicit vs. explicit AI (the ability to understand what’s going on in your system) or human-in-the-loop systems (responding to human input and making decisions). Those are your best next steps towards piecing together DRL. ,,,,,
4, The Unique Challenge of Working with Clinical Text in NLP, Working with high-intensity corpora like legal or clinical texts presents a unique challenge for NLP engineers. This is how to understand it., /index.html, Noah Caldwell-Gatsos, mailto:ncaldwellgatsos@gmail.com, @ Noah, 8/8/19, 12:40PM CST, /projects/ai/images/bp4front.jpg, The jargon used in clinical papers can often be the biggest hinderance to understanding them., Google Images," Doctors and lawyers are expected to keep up with a staggering amount of information. Each year, there‚Äôs a verifiable deluge - nearly 5 million - of scientific papers and legal opinions published[1]. It can be nearly impossible for these professionals to keep up with this information overload. But what if there was a better way to use this wealth of information rather than expect those interested to skim 2.5 million or so documents? Natural language processing - a subset of artificial intelligence - focuses on producing a solution to that problem. "," When you‚Äôre considering a use case for NLP, I often find that a lot of potential start-up innovators tend to focus on the headlines and don‚Äôt really have a good understanding of what NLP can do. This leads to poorly defined problem statements and proposal rejections. It helps to frame your task in the most explicit terms you can. Do you want a summarizer that can take in those 5 million articles and give you three-four sentences about them? Do you want a classifier: something that can read all of those 5 million articles and say arbitrarily (based on subjective criteria) that these articles are trash, and these articles are legitimate? Do you want a decision-maker - something that can read all those options - extract relevant information and put it to use? These questions and more are what you should be asking, because if you can‚Äôt frame the problem as explicitly as possible, artificial intelligence applications aren‚Äôt going to help your use case. This is because NLP is a super broad field, within an already broader field of artificial intelligence. A summarizer uses mechanisms called ‚Äòattention‚Äô and ‚Äòpointer-generators‚Äô that use linear algebra to vectorize information and assign more relevant examples higher weights to mimic human-based attention. A classifier normally falls under the broad category of sentiment analysis, where you give the system enough examples of whatever you want to classify and it begins to learn features about the data to allow it to distinguish between the classes. A decision-maker typically uses data to inform its decisions, so you‚Äôd want to use a named entity recognition model to pull out this data and a deep reinforcement learning model to put it to use. Involved in all of this is a ambiguity limiter, which allows you to determine the meaning of a word from its context. You can see how this begins to spiral out of control - hopefully illustrating the necessity of a cleanly defined problem statement from the very beginning. "," In this post specifically, I‚Äôm going to assume that the user wants to be able to extract useful data points and connect their outputs to a decision-making process that will replicate what a professional naturally does when they read through a white paper. So let‚Äôs break that process down - what do clinicians do when they read white papers? What do they look for? Your standard doctor typically wants to know the most up-to-date treatment options for whatever they‚Äôve diagnosed you with. They want them to be reliable, they want to limit the side effects, they want the materials needed to be available at the hospital or practice they work at, and they want the procedure to be as low cost as possible. Like I mentioned previously, the art of determining whether or not a paper is reliable is a separate task - it involves a fair amount of domain knowledge and a classifier model that can correctly analyze the corpora. You‚Äôll need a named entity recognizer, but only after ambiguous terms are properly lemmatized or grouped. You‚Äôll need a long-term memory deep reinforcement learning mechanism that allows it to make decisions by comparing each treatment to another, which means you‚Äôll need some kind of classifier that determines whether or not the sum total of a paper‚Äôs approach is better than another. That is way too much to cover in one post, so I‚Äôm going to just focus on the named entity recognition model and the ambiguity reduction model, since they‚Äôre linked. ","Named entity recognition and word sense disambiguation are two major issues in natural language processing. Both are difficult to achieve because of the ‚ÄòChicken and the Egg‚Äô data problem I‚Äôve mentioned in a previous <a href=‚Äúwww.ncaldwellgatsos.com/projects/ai/posts/ai_bp2.html‚Äù>post</a>. You essentially need to have the model working already if you want to train the model without significant manual labor. That manual labor isn‚Äôt easy either - you can‚Äôt just have an Amazon Turk do it for you on the cheap, you need to have a licensed professional go through and hand annotate all the data for you. Obviously, this isn‚Äôt a good solution. Luckily, there‚Äôs a lot of work being done in the field. There‚Äôs already some publicly available datasets that cover this exact problem through the ShARe project and the i2b2 initiative. There‚Äôs also plenty of work being done developing unsupervised models that are able to learn how to annotate documents themselves. Those unsupervised models are what my work focuses on, and here are some of the unique challenges that those working with clinical texts face."," Clinical papers include the following categories - discharge summaries, electrocardiogram, echocardiograms, radiology reports, white papers, discussions of a particular treatment, and reviews of a variety of treatments. For the sake of remaining HIPAA complaint, it‚Äôs probably best to focus on the latter three. One of the major issues with analyzing clinical white papers is their ambiguity. A single concept can not only be represented in a variety of ways, but it can also be a multi-token (i.e. have multiple words) concept, or be discontinuous mentions (i.e. requires multi-word or sentence relationship analysis to properly categorize. The objective with all NLP approaches is to group each concept into a defined label or identifier. The doctor is able to easily distinguish between drug names, procedure names, disorder names, and so on. The computer system needs to be able to do the same. In order to do that, the model needs to have a method of grouping all of these unique terms into groups. In my next post, I‚Äôll go over the several approaches towards taking all these unique terms and assigning them a CUI - a concept-unique identifier. ", NER, High-Intensity, NLP, AI, https://www.i2b2.org/, i2b2 Community Wiki, https://www.aclweb.org/anthology/S14-2007, SemEval-2014 Task 7, http://ncaldwellgatsos.com/projects/ai/posts/ai_bp2.html, The Chicken and the Egg of Named Entity Recognition, ,,,,,,,,,,
5,"DRL - Getting Started with Deep Reinforcement Learning Can Be a Beast, Here's a Way to Frame It", A lot of primers on DRL can be confusing and needlessly high level. There's a simpler way to begin your journey. , /index.html, Noah Caldwell-Gatsos, mailto:ncaldwellgatsos@gmail.com, @ Noah, 8/15/19, 4:11PM CST, /projects/ai/images/bp5front.jpg, Q-learning in action," Photo by <a href=""https://unsplash.com/@nasa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">NASA</a> on <a href=""https://unsplash.com/photos/NuE8Nu3otjo"">Unsplash</a>"," There's a wonderful metaphor to be found in fantasy fiction that applies to learning computer science, artificial intelligence, and machine learning. Often, the 'magic users' of a fantasy novel will act as gatekeepers to knowledge, making every tutorial and needlessly complicated for those that want to learn magic. This is intended to hoard this knowledge and power for themselves. I'm not trying to make every CS professor out to be an evil sorcerer, but‚Ä¶.just kidding, I just think it's really, really difficult to teach computer science - especially if you never wanted to be a teacher in the first place. I'm far from an expert, but I would like to make all these concepts a bit easier for everyone to understand. So here's how to frame deep reinforcement learning in your mind. "," The best way to understand and learn programming is to envision a project and figure out how to complete it. That means piecing together what kind of tools you'll need to use to finish it, and how to use those tools. In this example, I've created a fictional character named Marco. Marco's pretty good with the 'magical language' (i.e. Python), understands how to use the spells other people made for his own purposes (i.e. packages), and is beginning to be able to generalize how the 'magic world' (computing) works. Like all excellent wizards, Marco is lazy AF. His goal is to create a non-sentient agent that can do all his housework for him. But he has a major problem - creating spells takes a while! He's been working on his automated agent for a while and has realized that having a non-sentient butler means he has to personally create a unique spell for every single action in every single environment. Marco doesn't have time for this, he just wants a simple answer and he thought magic was supposed to make things easier! Marco wants his servant to be able to draw on Marco's own human experience but he doesn't want to just create another human. He wants the agent to master everything he knows and more in just an afternoon. Marco draws on his skills - he remembers that he can create mini-worlds, where he controls everything! Including time. Even though Marco isn't powerful enough to influence time in the real world, he can in this mini-world. But that doesn't help him, right? He needs to be able to use his agent in the real world, and having one that's limited to his fake world is useless. Marco spends the rest of the morning thinking and comes up with a solution! What if he takes the 'mind' of the agent and puts it in the fake world to gain experience AND THEN takes the trained mind back out and puts it back into his servant. He'd get exactly what he wants!"," Marco's just identified the foundations of reinforcement learning, a subset of AI. In this fictional world where magic's replaced programming, Marco's realized that in order to create an automated creature that can do advanced tasks, he needs it to gain experience. But gaining experience took Marco twenty-four years, and he still doesn't know everything! He doesn't have time to wait around for that. He needs to replicate the experience process at an immense scale. So he finds a workaround - make a fake world where he controls all the variables, set the variables to mimic the real world with some variations (i.e. a fake second = a fake hour/day/year) and puts the brain into the fake world. Once it's learned just as much, or more than Marco, he takes it out and he has a ready-made agent willing and capable of doing his accounting for him! Now that we kind of get a picture of what Marco's doing, let's abandon the magic metaphor to get into some of the more real world applications of what I just described.","In the next three sections, I want to cover the main aspects of deep reinforcement learning. The first is model construction. There are four main aspects that are essential to understand:<ol><li>The Goal</li><li>The World</li><li>The Action</li><li>The Reward</li></ol>These factors are pretty much all you need to build a deep reinforcement learning model. I'll go into each of them in a second, but I want to communicate an essential topic about data before we begin. "," Marco had it easy - with magic, all he had to do to create his fake world was use some kind of mystical energy available and think really hard about what he wanted to create. For us real worlders, we have different parameters. Data is the most important. There are three types of data out there: numerical, categorical, and both! Numerical data is the easiest to work with - computers can notice patterns quickly using mathematical operations. Categorical data is a bit more tricky. How can you begin to communicate to a computer that there's a qualitative difference between two different objects? How can you abstract this? NUMBERS. Feature engineering is a huge part of deep reinforcement learning and it's how a lot of the AI 'magic' happens. We just convert everything we find to numbers using a couple of use-specific methods. Feature engineering's super important, so I'll cover that one in a whole post later. The  important thing to understand is that everything will eventually be represented by numbers, and that's how the computers can pick up on patterns. ", DRL, AI, Machines, Programs, https://www.youtube.com/watch?v=zR11FLZ-O9M, MIT's Introduction to Deep Reinforcement Learning, https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184, A Gentle Introduction to Exploratory Data Analysis, https://github.com/jermwatt/machine_learning_refined, Machine Learning Refined (my prof's book)," With this in mind, let's talk about the goal. This is variable and can get SUPER complicated. Just look at self-driving cars. Your goal can be to get from point A to point B, but it also needs to not destroy the car, go off the road, go at a certain speed, not hit pedestrians, keep the passengers safe, etc. etc. Objective functions are the most important part of any deep learning model, and partially why it gets complicated. So let's start with something simple: a thermometer in a house. The goal is easy: keep the house temperature at 70 degrees Fahrenheit when people are home. The hard part then becomes defining the state (i.e. the world). "," The state space is the virtual world. There are three types of worlds you can create and explore. The simplest and easiest to work with are model-based reinforcement learning. MB-RL applies to games, especially ones with constant rules like Chess. The 'model' is the game rules. It can only take place on a special environment - (i.e. the chessboard). There are a limited number of things that can happen on the board (still an obnoxiously big number of possibilities for a human to work with), and it's why this is the easiest method to work with. LEARNING CURVE ALERT: Professors will confuse you by bringing up a concept called 'sample efficiency,' and it involves a LOT of math. To avoid going too far into decision trees and pruning, just know that there's a mathematical way to mimic how professional chess or Go players can 'feel' what a good move is, and then the computers do it even better. The fact that the computer doesn't have to plan out every single permutation of what could happen makes it 'efficient'. The next step is valued-based reinforcement learning. VB-RL tries to estimate the qualities of states through concepts called 'on and off policy'. Basically, the VB-RL guesses if its action might be good or bad for the world and its goal. Off policy means that the agent constantly tries to reevaluate its approach. This is 'sample inefficient,' because the model needs a lot of examples before it's actually able to do anything useful. They don't directly learn a strategy of action, they learn how good a course of action is and try and maximize that in their action. Occasionally, they'll flip a coin to try something new. On policy means that the agent will constantly change their approach given a scenario to determine whether or not their action is good. One of the most prominent algorithms to determine what's the best reward (I'll get to that in a second) is called Q-learning (named for Q, which is an abstraction for the action). With this, let's move on to the action:"," The action step will probably be more familiar - it's just an if function. An action says that if certain conditions are met, then the program will do something. Q-learning is a state-action function (i.e. it involves variables of both the state and action classes) that tries to maximize its reward. The reward helps produce the output (wait a minute, I'll get to rewards after this). Q-learning is an algorithm that basically says: use any strategy to guess a Q (i.e. an action) that maximizes the future reward. This is independent of the strategy you gave it at the beginning. The Q algorithm is a bit tricky to understand, but it's best to think of programmatically. It's basically a for-if-loop, i.e. after you've initialized the world, for a state and action in the world, perform the action, measure the reward, and update Q. Continue to do this if you haven't gotten to the optimal state, otherwise, terminate the loop. "," Finally, the reward function: it can get super wonky. Sometimes, especially if your data is all numeric, it can be really easy to determine. You've got a variable, 'x', and you just try and get it as high as it can go. In the real world, there's often many more variables involved, and they all affect each other (i.e. why it's a function). Many people like talking about the Cobra Effect. The Cobra Effect talks about an Indian Government policy where New Delhi tried to encourage people to bring in venomous snakes for money. People immediately started breeding these dangerous snakes to game the system. You do not want to train your system to adopt a solution that's worse than the problem. This is an edge case - the consequences of your system hopefully aren't dire - but this exact scenario can be abstracted to simply represent all negative outcomes that you don't want from a DRL model. A better example might be a robot arm that's being trained to place blocks on top of each other. The author of the code thought it'd be a good idea to make the 'reward' a measure of distance - put the cube as far as you can (the author thinking that the robot arm would stretch out all the way and place the cubes linearly on top of one another). The robot ended up learning how to chuck things as hard as it could against a wall. Rewards are a complicated task, and often aren't as easy as simply maximizing the unit of a variable. As I was writing this, I realized I was going a bit off tangent, so I'd like to cover rewards more in-depth in a future post. "," This is the basics behind deep reinforcement learning - it's all about creating a world, crafting an agent, giving it a set of actions it can take, and teaching it to maximize an arbitrary reward to gain experience and an idea of what actions will lead to what results. It's all a fascinating idea and hopefully I managed to convey its basics well. If you have any questions, let me know!", ,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,  ,,,